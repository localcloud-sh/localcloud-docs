---
title: Welcome to LocalCloud
description: AI Development at Zero Cost - Build production-ready AI applications on your local machine
---

<img
    className="block dark:hidden"
    src="/images/hero-light.svg"
    alt="LocalCloud Hero Light"
/>
<img
    className="hidden dark:block"
    src="/images/hero-dark.svg"
    alt="LocalCloud Hero Dark"
/>

## Build AI Applications Locally, Deploy Anywhere

LocalCloud revolutionizes AI development by providing a complete, local-first development environment that runs entirely on your machine. No cloud bills, no data privacy concerns, no complex configurations - just pure development productivity.

<CardGroup cols={2}>
    <Card
        title="Get Started in 30 Seconds"
        icon="rocket"
        href="/getting-started/quickstart"
    >
        Initialize, configure, and launch your AI stack with just three commands
    </Card>
    <Card
        title="Zero Cloud Costs"
        icon="dollar-sign"
        href="/concepts/services"
    >
        Everything runs locally - no API fees, no usage limits, no surprise bills
    </Card>
    <Card
        title="Production-Ready Templates"
        icon="layer-group"
        href="/examples/ai-chat"
    >
        Pre-built backends for chat assistants, RAG systems, and speech processing
    </Card>
    <Card
        title="Runs on 4GB RAM"
        icon="microchip"
        href="/getting-started/installation"
    >
        Optimized models and efficient resource management for any laptop
    </Card>
</CardGroup>

## Quick Start

Get your first AI application running in under 5 minutes:

```bash
# Install LocalCloud (macOS/Linux with Homebrew)
brew install localcloud-sh/tap/localcloud

# Or use the install script
curl -fsSL https://raw.githubusercontent.com/localcloud-sh/localcloud/main/scripts/install.sh | bash

# Create a new project
lc init my-assistant

# Configure and start services
cd my-assistant
lc setup
lc start
```

Your AI services are now running locally! Check out the [Quickstart Guide](/getting-started/quickstart) for detailed instructions.

## Why LocalCloud?

### üè¢ Enterprise POCs Without The Red Tape
Waiting 3 weeks for cloud access approval? Your POC could be done by then. LocalCloud lets you build and demonstrate AI solutions immediately, no IT tickets required.

### üì± Mobile Demos That Actually Work
Present from your phone to any client's screen. Built-in tunneling means you can demo your AI app from anywhere - coffee shop WiFi, client office, or conference room.

### üí∏ No More Forgotten Demo Bills
We've all been there - spun up a demo, showed the client, forgot to tear it down. With LocalCloud, closing your laptop *is* shutting down the infrastructure.

### üéì Perfect for Learning
Students and developers can experiment with cutting-edge AI models without worrying about costs or quotas. Build, break, and rebuild as much as you want.

## Core Features

<AccordionGroup>
    <Accordion title="One-Command Setup" icon="terminal">
        LocalCloud's interactive CLI guides you through the entire setup process. Choose from pre-built templates or customize your stack component by component.
    </Accordion>
    <Accordion title="Pre-built Templates" icon="shapes">
        Start with production-ready configurations for common use cases:
        - **Chat Assistant**: Conversational AI with memory and context
        - **RAG System**: Document Q&A with vector search
        - **Speech Processing**: Whisper STT + TTS pipelines
    </Accordion>
    <Accordion title="Optimized AI Models" icon="brain">
        Carefully selected models that balance performance and resource usage:
        - **Llama 3.2**: Best overall performance for chat
        - **Qwen 2.5**: Excellent for coding tasks
        - **Nomic Embed**: Efficient text embeddings
        - **Whisper**: State-of-the-art speech recognition
    </Accordion>
    <Accordion title="Complete Infrastructure" icon="server">
        Everything you need for production AI applications:
        - PostgreSQL with pgvector for embeddings
        - Redis for caching and queues
        - MinIO for S3-compatible storage
        - Ollama for model serving
    </Accordion>
</AccordionGroup>

## Join the Community

<CardGroup cols={3}>
    <Card title="GitHub" icon="github" href="https://github.com/localcloud-sh/localcloud">
        Star us on GitHub and contribute to the project
    </Card>
    <Card title="Discord" icon="discord" href="https://discord.gg/localcloud">
        Join our Discord community for support and discussions
    </Card>
    <Card title="Twitter" icon="x-twitter" href="https://twitter.com/localcloudai">
        Follow us for updates and announcements
    </Card>
</CardGroup>

## Ready to Start Building?

<CardGroup cols={2}>
    <Card
        title="Installation Guide"
        icon="download"
        href="/getting-started/installation"
    >
        Install LocalCloud on macOS, Linux, or Windows
    </Card>
    <Card
        title="Browse Templates"
        icon="rectangle-terminal"
        href="/examples/ai-chat"
    >
        Explore pre-built application templates
    </Card>
</CardGroup>